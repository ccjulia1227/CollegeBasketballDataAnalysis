---
title: "College Basketball Data Analysis"
author: "Xixi"
date: '2022-12-30'
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(data.table)
library(corrplot)
library(ggcorrplot)
library(gridExtra)
library(caret)
library(kableExtra)

cbb <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb.csv")
cbb13 <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb13.csv")
cbb14 <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb14.csv")
cbb15 <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb15.csv")
cbb16 <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb16.csv")
cbb17 <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb17.csv")
cbb18 <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb18.csv")
cbb19 <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb19.csv")
cbb20 <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb20.csv")
cbb21 <- read_csv("/Users/juliadu//College/UNC SAIL/College Basketball Dataset/cbb21.csv")

# Data from the 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, and 2021 Division I college basketball seasons.
# cbb.csv has seasons 2013-2019 combined
# The 2020 season's data set is kept separate from the other seasons, because there was no postseason due to the Coronavirus.
# The 2021 data is from 3/15/2021 and will be updated and added to cbb.csv after the tournament

```

```{r, include=FALSE}
# roughly explore the data
glimpse(cbb)
unique(cbb$SEED)
cbb
```

```{r, include = FALSE}
cbb13
unique(cbb$TEAM) #Seed的区间是1-16，可以有重合的Seed，team总数是68
cbb14
cbb20 # 因为疫情，没有seed和postseason数据
cbb21 # 21年没有postseason的数据，但是有seed
unique(cbb13$CONF)
length(unique(cbb13$CONF))
```


# INTRODUCTION

*篮球比赛的广泛性和对于人们的影响之大。

*所以研究篮球数据的重要性不言而喻。

This paper will focus on solving the two questions related to the two variables stated above: Do the basketball levels differ between `Conferences`? If so, what may cause the differences? And how can we predict a team's `Seed` based on the its existing basketball game performance data of the regular season?

Our analysis aims to identify the basketball performance indicators which help the team ace in the regular season and assist the coaches in designing targeted training programs, improving teams' competitiveness.

# DATA

My analysis was done on the data from [Kaggle](https://www.kaggle.com/datasets/andrewsundberg/college-basketball-dataset?select=cbb19.csv) named College Basketball Dataset by Andrew Sunberg. The data is from the 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, and 2021 Division I college basketball seasons. Due to Covid-19, there was no postseason for the 2020 season's data set, so I did not include the data from 2020 in my analysis. I added a `YEAR` column to each dataset from 2013 to 2021, except 2020. I also created a `W_ratio` column, which is the total winning games to the total number of games played for a specific team in the regular of a particular year. Then I combined all the datasets to form a combined dataset named *cbb_all*. For the convenience of performing the quantitative calculation, I intended to convert the professional expressions in the `POSTSEASON` into numbers: *R68* to *68*, *R64* to *64*, *R32* to *32*, *S16* to *16*, *E8* to *8*, *F4* to *4*, *2ND* to *2*, and *Champions* to *1*. 

I will interpret all the variables in the modified dataset in the following table:

```{r, echo=FALSE}
# 以下正式开始数据分析
cbb13_new = cbb13 %>% 
  mutate(YEAR = 2013)
cbb14_new = cbb14 %>% 
  mutate(YEAR = 2014)
cbb15_new = cbb15 %>% 
  mutate(YEAR = 2015)
cbb16_new = cbb16 %>% 
  mutate(YEAR = 2016)
cbb17_new = cbb17 %>% 
  mutate(YEAR = 2017)
cbb18_new = cbb18 %>% 
  mutate(YEAR = 2018)
cbb19_new = cbb19 %>% 
  mutate(YEAR = 2019)
cbb20_new = cbb20 %>% 
  mutate(YEAR = 2020)
cbb21_new = cbb21 %>% 
  mutate(YEAR = 2021)

cbb_list = list(cbb13_new,cbb14_new,cbb15_new,cbb16_new,cbb17_new,cbb18_new,cbb19_new,cbb21_new)
cbb_all = rbindlist(cbb_list,use.names = TRUE, fill=TRUE)
```

```{r, echo=FALSE}
cbb_all$POSTSEASON[cbb_all$POSTSEASON == 'R68'] <- '68'
cbb_all$POSTSEASON[cbb_all$POSTSEASON == 'R64'] <- '64'
cbb_all$POSTSEASON[cbb_all$POSTSEASON == 'R32'] <- '32'
cbb_all$POSTSEASON[cbb_all$POSTSEASON == 'S16'] <- '16'
cbb_all$POSTSEASON[cbb_all$POSTSEASON == 'E8'] <- '8'
cbb_all$POSTSEASON[cbb_all$POSTSEASON == 'F4'] <- '4'
cbb_all$POSTSEASON[cbb_all$POSTSEASON == '2ND'] <- '2'
cbb_all$POSTSEASON[cbb_all$POSTSEASON == 'Champions'] <- '1'
cbb_all$CONF[cbb_all$CONF == 'ind'] <- 'Ind'

cbb_all <- cbb_all %>% 
        rename( "two_P_O" = "2P_O",
                "two_P_D" = "2P_D",
                "three_P_O" = "3P_O",
                "three_P_D" = "3P_D")

cbb_all = cbb_all %>% 
  mutate(W_ratio = W/G) %>% 
  select('YEAR','TEAM','SEED','POSTSEASON','CONF','G','W','W_ratio','ADJOE','ADJDE','BARTHAG','EFG_O','EFG_D','TOR','TORD','ORB','DRB','FTR','FTRD','two_P_O','two_P_D','three_P_O','three_P_D','ADJ_T','WAB')

cbb_all$POSTSEASON <- as.numeric(cbb_all$POSTSEASON)

```

```{r,echo=FALSE}
Variable <- as.vector(c(
  'YEAR',
  'TEAM',
  'SEED',
  'POSTSEASON',
  'CONF',
  'G',
  'W',
  'W_ratio',
  'ADJOE',
  'ADJDE',
  'BARTHAG',
  'EFG_O',
  'EFG_D',
  'TOR',
  'TORD',
  'ORB',
  'DRB',
  'FTR',
  'FTRD',
  'two_P_O',
  'two_P_D',
  'three_P_O',
  'three_P_D',
  'ADJ_T',
  'WAB')
)

Definition <- as.vector(c(
  'Year range from 2013 to 2021 (except 2020)',
  'The Division I college basketball school',
  'Seed in the NCAA March Madness Tournament',
  'Round where the given team was eliminated or where their season ended (68 = First Four, 64 = Round of 64, 32 = Round of 32, 16 = Sweet Sixteen, 8 = Elite Eight, 4 = Final Four, 2 = Runner-up, 1 = Winner of the NCAA March Madness Tournament for that given year)',
  'The Athletic Conference in which the school participates in (A10 = Atlantic 10, ACC = Atlantic Coast Conference, AE = America East, Amer = American, ASun = ASUN, B10 = Big Ten, B12 = Big 12, BE = Big East, BSky = Big Sky, BSth = Big South, BW = Big West, CAA = Colonial Athletic Association, CUSA = Conference USA, Horz = Horizon League, Ivy = Ivy League, MAAC = Metro Atlantic Athletic Conference, MAC = Mid-American Conference, MEAC = Mid-Eastern Athletic Conference, MVC = Missouri Valley Conference, MWC = Mountain West, NEC = Northeast Conference, OVC = Ohio Valley Conference, P12 = Pac-12, Pat = Patriot League, SB = Sun Belt, SC = Southern Conference, SEC = South Eastern Conference, Slnd = Southland Conference, Sum = Summit League, SWAC = Southwestern Athletic Conference, WAC = Western Athletic Conference, WCC = West Coast Conference)',
  'Number of games played',
  'Number of games won',
  'Number of games played to Number of games won',
  'Adjusted Offensive Efficiency (An estimate of the offensive efficiency (points scored per 100 possessions) a team would have against the average Division I defense)',
  'Adjusted Defensive Efficiency (An estimate of the defensive efficiency (points allowed per 100 possessions) a team would have against the average Division I offense)',
  'Power Rating (Chance of beating an average Division I team)',
  'Effective Field Goal Percentage Shot',
  'Effective Field Goal Percentage Allowed',
  'Turnover Percentage Allowed (Turnover Rate)',
  'Turnover Percentage Committed (Steal Rate)',
  'Offensive Rebound Rate',
  'Offensive Rebound Rate Allowed',
  'Free Throw Rate (How often the given team shoots Free Throws)',
  'Free Throw Rate Allowed',
  'Two-Point Shooting Percentage',
  'Two-Point Shooting Percentage Allowed',
  'Three-Point Shooting Percentage',
  'Three-Point Shooting Percentage Allowed',
  'Adjusted Tempo (An estimate of the tempo (possessions per 40 minutes) a team would have against the team that wants to play at an average Division I tempo)',
  'Wins Above Bubble (The bubble refers to the cut off between making the NCAA March Madness Tournament and not making it)')
)


data_all <- data.frame(Variable, Definition)
data_table <- data_all %>%
  kbl(caption = "Data") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = c("striped", "hold_position")) # cell与cell之间自动分行，有分割线
  
data_table

```



# RESULTS

> Question 1: Do the basketball levels differ between `Conferences`? If so, what may cause the differences?

For a person who doesn't know much about basketball, the indicators to measure a basketball team's level are the `SEED` at the end of the postseason and the ranking in the finals, the `POSTSEASON` in our dataset. Unfortunately, since we currently don't have the performance data of the postseason (we only have the results), it is nearly impossible to perform the analysis. In this manner, I chose to focus on the `SEED` and wanted to explore the patterns behind it. I counted the total number of teams in each `SEED` from 2013 to 2021 (excluding 2020) and grouped them by `CONF` (Conference). It is worth mentioning that I filtered all the *NA* values because they were not significant. I utilized a *tile plot*, and the numbers on the tile represent the number of teams in the specific `SEED` of a particular `CONF`. For example, the bottom left corner tile demonstrates that *ACC has total 9 teams in Seed 1 from 2013 to 2021 (excluding 2020)*. The darker the color of the tile, the larger the number of teams of the conference in the seed. We know that the team getting *Seed 1* is more competitive than that gets *Seed 2*, and the team getting *Seed 2* is more competitive than that gets *Seed 3*, etc. Thus, conferences with higher basketball levels have more teams in the top seeds, *Seed 1*, *Seed 2*, *Seed 3*, and *Seed 4*, having greater chances of reaching the finals.

```{r, fig.align='center',fig.height=8,fig.width=8, echo=FALSE,warning=FALSE}
# seed跟所在的conference有没有关系？或许存在某个conference占据很多席位的现象？与实力的关系？

cbb_all_1 <- cbb_all %>% 
  count(CONF,SEED) %>% 
  arrange(CONF,SEED) %>%  
  filter(SEED != "NA")

p_seed <- ggplot(cbb_all_1, aes(CONF, SEED, fill = n)) +
  geom_tile(color = "black") +
  geom_text(aes(label=n)) +
  scale_fill_continuous(low= "floralwhite", high= "darkblue") +
  scale_y_continuous(breaks=seq(1,16,1)) +
  xlab("Conference")+ylab("Seed")+ggtitle("Total Number of Teams in Seed 1 to Seed 16 For Every Conference Over 2013-2021") + # 2020的数据除外
  coord_fixed() +
  coord_flip() +
  theme_bw()
  
p_seed

# 在tile上增加数字
# 改为深色代表“大”，浅色代表“小”,空白代表NA(而并非将NA filter掉)

# 从历年数据(2013-2021,2020除外)来看，ACC,B10,B12在seed上有绝对的优势。
# MEAC, NEC和SWAC排名比较落后
# GWC和Ind不在上述的表格中了，因为这两个conferences从来没有team获得seed席位。一共是33个conference，外加一个independent，共计34个conference；去掉没有排上号的两个conference，还剩下图上的32个。

```


Based on the colors and numbers of the tile plot, conferences ACC, B10, B12, and BE have an absolute advantage in reaching the top seeds, while conferences MEAC, NEC, and SWAC are less competitive in more likely to get *SEED 16*. Additionally, conference GWC and Ind group (independent teams) are not even on the plot, which indicates that they never made it to the finals. Thus, we can roughly conclude that conferences like ACC, B10, and B12 are of higher basketball level than conferences like MEAC, NEC, SWAC, and GWC. 

```{r,include=FALSE}
# 按照CONF分组，将performance data绘制在box plot上，寻找相差特别大的indicators。
# 分析..

box_1 <-ggplot(cbb_all, aes(x=CONF, y=G)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5)) + labs(title="Boxplot of G among Conferences",x= "CONF", y = "G")
box_1
# 不选择
# B10 is the highest, 
# ACC, B12, BE, P12, SEC high..
# Ind lowest
# GWC, Ivy, low

```


```{r,include=FALSE}
box_2 <- ggplot(cbb_all, aes(x=CONF, y=W)) + 
  geom_boxplot() + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5)) + 
  labs(title="Boxplot of W among Conferences",x= "Conference", y = "W")

box_2
# 选择
# ACC, B10, B12, BE higher
# Ind lower

```

```{r,include=FALSE}
box_3 <- ggplot(cbb_all, aes(x=CONF, y=W_ratio)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5))+ labs(title="Boxplot of W_ratio among Conferences",x= "CONF", y = "W_ratio")
box_3
# 不选择
# ACC, B10, B12, BE high 
# Ind, MEAC lower

```


```{r, fig.align='center',include=FALSE}
box_4 <- ggplot(cbb_all, aes(x=CONF, y=ADJOE)) + 
  geom_boxplot() + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5))+ 
  labs(title="Boxplot of ADJOE among Conferences",x= "Conference", y = "ADJOE")
box_4
# 选择
# ACC, B12 highest, 
# B10, BE high
# GWC, SWAC low

```

```{r,include=FALSE}
box_5 <- ggplot(cbb_all, aes(x=CONF, y=ADJDE)) + 
  geom_boxplot() + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5))+ 
  labs(title="Boxplot of ADJDE among Conferences",x= "Conference", y = "ADJDE")
box_5
# 选择
# B10, B12 lowest..
# ACC, BE, SEC low

```

```{r,include=FALSE}
box_6 <- ggplot(cbb_all, aes(x=CONF, y=BARTHAG)) + 
  geom_boxplot() + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5)) + 
  labs(title="Boxplot of BARTHAG among Conferences",x= "Conference", y = "BARTHAG")
box_6
# 选择
# B12 highest
# B10, ACC high

```


```{r,include=FALSE}
box_7 <- ggplot(cbb_all, aes(x=CONF, y=EFG_O)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5)) + labs(title="Boxplot of EFG_O among Conferences",x= "CONF", y = "EFG_O")
box_7
# 平均

# GWC, SWAC lowest
# 其他比较平均

```

```{r,include=FALSE}
box_8 <- ggplot(cbb_all, aes(x=CONF, y=EFG_D)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5))+ labs(title="Boxplot of EFG_D among Conferences",x= "CONF", y = "EFG_D")
box_8
# 平均
# 整体比较平均

```


```{r,include=FALSE}
box_9 <- ggplot(cbb_all, aes(x=CONF, y=TOR)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5))+ labs(title="Boxplot of TOR among Conferences",x= "CONF", y = "TOR")
box_9
# 平均

```


```{r,include=FALSE}
box_10 <- ggplot(cbb_all, aes(x=CONF, y=TORD)) +
  geom_boxplot() + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5))+ 
  labs(title="Boxplot of TORD among Conferences",x= "Conference", y = "TORD")
box_10
# 选择
# GWC highest (extreme)
# ind high

```


```{r,include=FALSE}
box_11 <- ggplot(cbb_all, aes(x=CONF, y=ORB)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5))+ labs(title="Boxplot of ORB among Conferences",x= "CONF", y = "ORB")
box_11
# 平均
# 相对平均，Sum略低

```


```{r,include=FALSE}
box_12 <- ggplot(cbb_all, aes(x=CONF, y=DRB)) + 
  geom_boxplot() + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5))+ 
  labs(title="Boxplot of DRB among Conferences",x= "Conference", y = "DRB")
box_12
# 选择
# Ind, GWC, MEAC highest
# 其他比较平均

```


```{r,include=FALSE}
box_13 <- ggplot(cbb_all, aes(x=CONF, y=FTR)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5))+ labs(title="Boxplot of FTR among Conferences",x= "CONF", y = "FTR")
box_13
# 平均
# Ind slightly higher
# Pat lower

```


```{r,include=FALSE}
box_14 <- ggplot(cbb_all, aes(x=CONF, y=FTRD)) + 
  geom_boxplot() + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5))+ 
  labs(title="Boxplot of FTRD among Conferences",x= "Conference", y = "FTRD")
box_14
# 选择
# Ind highest (extreme)
# GWC high
# 其他比较平均

```


```{r,include=FALSE}
box_15 <- ggplot(cbb_all, aes(x=CONF, y=two_P_O)) + 
  geom_boxplot() + 
  theme_bw() + 
  theme(axis.text.x=element_text(angle=60,vjust=0.5))+ 
  labs(title="Boxplot of two_P_O among Conferences",x= "Conference", y = "two_P_O")
box_15
# 选择
# GWC lowest (extreme).       
# SWAC, Ind, MEAC low.  
# 其他比较平均.    

```


```{r,include=FALSE}
box_16 <- ggplot(cbb_all, aes(x=CONF, y=two_P_D)) + 
  geom_boxplot() + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5))+ 
  labs(title="Boxplot of two_P_D among Conferences",x= "Conference", y = "two_P_D")
box_16
# 选择
# ASun relatively high
# Amer, SEC, ACC, B10, B12, P12 relatively low
# 其余波动不大

```


```{r,include=FALSE}
box_17 <- ggplot(cbb_all, aes(x=CONF, y=three_P_O)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5))+ labs(title="Boxplot of three_P_O among Conferences",x= "CONF", y = "three_P_O")
box_17
# 平均
# WAC lowest
#. EAC low
# Sum slightly higher

```


```{r,include=FALSE}
box_18 <- ggplot(cbb_all, aes(x=CONF, y=three_P_D)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5))+ labs(title="Boxplot of three_P_D among Conferences",x= "CONF", y = "three_P_D")
box_18
# 平均
# Sum, BSky relatively higher
# 其余波动不大

```


```{r,include=FALSE}
box_19 <- ggplot(cbb_all, aes(x=CONF, y=ADJ_T)) + geom_boxplot() + theme(axis.text.x=element_text(angle=45,vjust=0.5))+ labs(title="Boxplot of ADJ_T among Conferences",x= "CONF", y = "ADJ_T")
box_19
# 平均
# Asun slightly higher

```


```{r, include=FALSE}
box_20 <- ggplot(cbb_all, aes(x=CONF, y=WAB)) + 
  geom_boxplot() + 
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5))+ 
  labs(title="Boxplot of WAB among Conferences",x= "Conference", y = "WAB")
box_20
# 选择
# 分布参差不齐
# B12 highest (>0)
# ACC, B10, BE also very high (around 0)
# P12, SEC, A10 high (-5 - 0)
# 其他 (-15 - -5)


```


To explore the reasons for the differences in `SEED`, I implemented boxplots to illustrate the distribution of the game performance indicators among conferences. I will specifically talk about the characteristic ones in the following:  

```{r, fig.align='center',fig.width=8,fig.height=8,echo=FALSE}
grid.arrange(box_2,box_4,ncol=1,nrow=2) # high
grid.arrange(box_6,box_20,ncol=1,nrow=2) # high
grid.arrange(box_5,box_16,ncol=1,nrow=2) # low 

```

Among these 6 boxplots, conferences ACC, B10, B12, and BE are likely to have higher values in `W`, `ADJOE`, `BARTHAG`, and `WAB` than other conferences shown in the first 4 boxplots while having lower values in `ADJDE` and `two_P_D` than other conferences in the last 2 boxplots. 

```{r, fig.align='center',fig.width=8,fig.height=8,echo=FALSE}
grid.arrange(box_12,box_10,ncol=1,nrow=2) 
grid.arrange(box_14,box_15,ncol=1,nrow=2) 

# 10,12,14 high
# 15 low
```

For these 4 boxplots, conferences GWC, MEAC, NEC, and SWAC and Ind group are likely to have higher values in `DRB`, `TORD`, and `FTRD` than other conferences in the first 3 boxplots while having lower values in `two_P_O` than other conferences shown by the last one.

The distributions of the remaining game performance indicators among conferences do not fluctuate much, so I did not include them in the analysis. Those game performance indicators mentioned above may be the reasons causing the differences in the `SEED` among the conferences, but further data analysis is still needed.


> Question 2: How can we predict a team's `Seed` based on the its existing basketball game performance data of the regular season?

## 寻找影响seed的因素

seed与postseason的关系？
(需要思考出一种方法量化postseason的排名数据 重要‼️

一些文献依据
The investigation in this area has been focused, traditionally, on men’s basketball teams (Ibáñez et al., 2003; Trninić et al., 2002). These topics have been widely studied and some important trends have been found: fields goals and defensive rebounds are the game performance indicators that best discriminate between winning and losing teams in basketball (Ibáñez et al., 2003; Ittenbach and Esters, 1995; Karipidis et al., 2001). Sampaio et al. (2010) suggest that winning teams performance is due to the achievement of more opportunities to attempt field-goals and also to the improvement in the decision making of the players of the winning teams as well as a better strategic and tactical environment. More recent studies have been exploring contextual factors such as game location (home and away), game type (regular season and playoff), game final score differences (close, balanced and unbalanced games), players’ gender (men and women), level of competition (Euroleague, National Basketball Association, etc.) and age (senior and junior) to establish better understanding of performance analysis (Lorenzo et al., 2010).

The study of game performance as a function of the differences in the final score of the game is becoming an important variable to consider. Available literature has analyzed categories separately (Sampaio and Janeira, 2003; Gómez et al., 2008). In balanced games (equal or below 12 points) the game performance indicators that discriminated between winners and losers were defensive rebounds (Gómez et al., 2008), missed 3-point field goals (Gómez et al., 2009), field goals percentages and defensive rebounds (Janeira et al., 1996) and in unbalanced games (between 13 and 28 points of final score difference). Sampaio and Janeira (2003) concluded that losing teams performed poorly in all performance indicators. Gómez et al. (2008) found that successful 2 points field-goals, defensive rebounds and assists discriminated between winning and losing teams.

Conversely, only a few studies have analyzed performance in function of a competition phase and never with high-level basketball competition. In fact, most of professional leagues are organized in two phases: firstly, all the teams compete against each other twice in a double round, with the aim of add up victories to classification. Finally, only the top classified teams (usually the best 8 teams of the regular season) play-off for the championship. The qualifying round is organized in such a way that best classified teams compete against the worst classified team and so on in a knockout round the best of three or five games. Thus, winning a game has a different importance: while in the regular season you can lose three games in a row and remain in competition, if you lose two of three, or three of five playoff games in a row you will be eliminated. Sampaio and Janeira (2002) studied Portuguese Professional Basketball League, which is a lower level competition according to the International Basketball Federation (Gómez et al., 2008). They pointed out that in the playoff games there were less ball possessions (71.16) than in the regular season games (74.75). Hence, the game rhythm was slower with a direct impact on the points per game, reducing the final score. There are no more studies inspecting this contrast between regular season and playoff games. Therefore, the aim of the present study was to identify basketball game performance indicators, which best discriminate between winners and losers in regular season and playoff games.


To develop the best approach for predicting the `SEED`, we wanted to create effective models that would indicate how the game performance indicators might be used to understand the patterns. Therefore, I calculated the correlation coefficients (r) of `SEED` and all the game performance indicators variables and utilized a scatterplot to display them as follows:

```{r,echo=FALSE}
library(DT)
cbb_noteam <- subset(cbb_all, select = -c(TEAM,CONF))
cbb_corr <- na.omit(cbb_noteam)
# cor(cbb_corr[,"SEED"], cbb_corr) %>% as.data.frame() %>% datatable()
# 转换成dataframe
# KableExtra - package
# DT - package datatable()
# Hmisc - package rcorr()

# seed与postseason的关系？
# (需要思考出一种方法量化postseason的排名数据 重要‼️
```

```{r,fig.align='center',echo=FALSE}
# fig.align='center',fig.height=4, fig.width=4
# ggcorrplot(cor(cbb_corr[,"SEED"], cbb_corr),lab = TRUE) + coord_flip()
cbb_corr_df <- as.data.frame(cor(cbb_corr[,"SEED"], cbb_corr))
cbb_corr_df <- cbb_corr_df[,-c(2)]
cbb_corr_df_arg <- cbb_corr_df %>% 
  gather(`YEAR`,`POSTSEASON`,`G`,`W`,`W_ratio`,`ADJOE`,`ADJDE`,`BARTHAG`,`EFG_O`,`EFG_D`,`TOR`,`TORD`,`ORB`,`DRB`,`FTR`,`FTRD`,`two_P_O`,`two_P_D`,`three_P_O`,`three_P_D`,`ADJ_T`,`WAB`, key = "performance", value = "correlation") %>% 
  arrange(desc(correlation))

ggplot(cbb_corr_df_arg, aes(performance, correlation)) + 
  geom_point() +
  geom_hline(yintercept= 0.3, color = "pink") +
  geom_hline(yintercept= -0.3, color = "pink") +
  labs(title="Correlation Coefficients of Seed and other Performance Indicators",x= "Performance Indicators", y = "Correlation Coefficients") +
  theme_bw() +
  theme(axis.text.x=element_text(angle=60,vjust=0.5)) +
  scale_y_continuous(breaks=seq(-1,1,0.25)) 

#cbb_corr_df %>%
#  kbl() %>%
#  kable_styling()

# 增加y轴的范围，0-1
# 可以考虑一下只画seed和所有变量的关系，重点关注seed和postseason的关系
# knit之后效果不好，需要调整；或许还是采取之前的策略
# 主要关注点在seed(主要是想通过regular season的数据来预测seed)和postseason(探究seed和postseason的关系)上！


# positively correlated: seed and postseason (r= 0.620798514), seed and ADJDE (r= 0.697441023), EFG_D (r= 0.38708232), two_P_D (r= 0.38995196) 
# negatively correlated: seed and WAB (r= -0.918860161强相关), seed and BARTHAG (r= -0.826583963强相关), seed and ADJOE (r= -0.745242957), seed and W (r= -0.649496188), G (r= -0.58732506) ,W_ratio (r= -0.49766516)

```

On the scatterplot, certain points draw my attention:

The variables strongly positively correlated to `SEED`: `ADJDE` (r= 0.697441023) and `POSTSEASON` (r= 0.62079851). 

The variables strongly negatively correlated to `SEED`: `WAB` (r= -0.918860161), `BARTHAG` (r= -0.826583963), `ADJOE` (r= -0.745242957), and `W` (r= -0.649496188).

One interesting point is that the correlation coefficient of `SEED` and `YEAR` is -0.002698 (nearly 0, indicating that `YEAR` does not influence which teams get into finals.

I draw two pink horizontal lines (r= 0.3 and r= -0.3, respectively) on the scatterplot, serving as a watershed between a weak and moderated relationship. Then I will include all the variables whose correlation coefficients are either greater or equal to 0.3 (< 1) or less or equal to -0.3 (> -1), which are `ADJDE`, `EFG_D`, `two_P_D`, `WAB`, `BARTHAG`, `ADJOE`, `W`, `G`, and `W_ratio`, into my model to predict the `SEED` as the modified *full model*. I did not include `POSTSEASON` because it has no influence on the `SEED` variable.

I created 4 models for testing. The first model was empty and relied purely on the average `SEED` from 2013 to 2021 except 2020. The next model contained all predictors (all the game performance indicators). Using the *Both-direction Stepwise Model Selection* method on the full model, our third model picked the best predictors using AIC (AIC=346.59). Likewise, the last model was chosen from the modified *full model* mentioned above using the *Both-direction Stepwise Model Selection* method based on the AIC (AIC=334.78). The third model is -6.44869443+ -0.82091537* `WAB` + 8.24557772* `W_ratio` + 0.17145355* `two_P_D` + 15.13874804* `BARTHAG` + -0.30972253* `ADJOE` + 0.36305352* `ADJDE` + -0.07591390* `ADJ_T` + 0.03701627* `FTRD` + -0.13172055* `EFG_D`, while the last model is -4.33766090 + -0.82986380* `WAB` + 7.99275351* `W_ratio` + -0.07521933* `G` + -0.29202360* `ADJOE` + 14.16200872* `BARTHAG` + 0.33506762* `ADJDE`.

I did *Cross-Validation* on the original dataset, splitting the data into the training dataset and testing dataset with the ratio of 0.8:0.2. I built the models based on the training dataset and measured each model using mean average error (MAE) on the testing dataset. The results are shown in the table below:

```{r,echo=FALSE,warning=FALSE}
# set.seed()
# train data
# test data
# lm() function with summary()
# goal: minimize MSE and MAE

#K-fold cross-Validation
#https://www.geeksforgeeks.org/cross-validation-in-r-programming/

# setting seed to generate a reproducible random sampling
cbb_model = cbb_corr
set.seed(6)

# creating training data as 80% of the dataset
random_sample <- createDataPartition(cbb_model$SEED,
                                p = 0.8, list = FALSE)
# generating training dataset from the random_sample
training_dataset  <- cbb_model[random_sample, ]
# generating testing dataset from rows which are not included in random_sample
testing_dataset <- cbb_model[-random_sample, ]

# Building the model
model_empty <- lm(SEED ~ 1, data = training_dataset)
# Predicting the target variable
predictions <- predict(model_empty, testing_dataset)
# Computing model performance metrics
metrics_empty <- data.frame( R2 = R2(predictions, testing_dataset$SEED),
            RMSE = RMSE(predictions, testing_dataset$SEED),
            MAE = MAE(predictions, testing_dataset$SEED))

# Building the model
model_full <- lm(SEED ~., data = training_dataset)
# Predicting the target variable
predictions <- predict(model_full, testing_dataset)
# Computing model performance metrics
metrics_full <- data.frame( R2 = R2(predictions, testing_dataset$SEED),
            RMSE = RMSE(predictions, testing_dataset$SEED),
            MAE = MAE(predictions, testing_dataset$SEED))

# K-fold Cross-Validation
# train_control <- trainControl(method = "cv",
#                              number = 10)
#model <- train(SEED ~., data = cbb_model,
#               method = "lm",
#               trControl = train_control)
#print(model)

```

```{r,fig.align='center',fig.width=8,fig.height=8,echo=FALSE}
cbb_train_1 <- subset(training_dataset, select = c(SEED,G,W,W_ratio,ADJOE,ADJDE,BARTHAG,EFG_D,two_P_O,WAB))
cbb_train_2 <- subset(training_dataset, select = -c(YEAR,POSTSEASON))

#define intercept-only model
intercept_only_1 <- lm(SEED ~ 1, data= cbb_train_1)
#define model with all predictors
all_1 <- lm(SEED ~ ., data= cbb_train_1)
#perform backward stepwise regression
model_both_1 <- step(intercept_only_1, direction='both', scope=formula(all_1), trace=0) #表现最好的model(analysis中的最后一个model)
#view results of backward stepwise regression
#model_both_1$anova
#view final model
# model_both_1$coefficients

#define intercept-only model
intercept_only_2 <- lm(SEED ~ 1, data= cbb_train_2)
#define model with all predictors
all_2 <- lm(SEED ~ ., data= cbb_train_2)
#perform backward stepwise regression
model_both_2 <- step(intercept_only_2, direction='both', scope=formula(all_2), trace=0) #表现第二好的model(analysis中的第三个model)
#view results of backward stepwise regression
#model_both_2$anova
#view final model
#model_both_2$coefficients

# Predicting the target variable
predictions_1 <- predict(model_both_1, testing_dataset)
# Computing model performance metrics
metrics_both_1 <- data.frame( R2 = R2(predictions_1, testing_dataset$SEED),
            RMSE = RMSE(predictions_1, testing_dataset$SEED),
            MAE = MAE(predictions_1, testing_dataset$SEED))

# Predicting the target variable
predictions_2 <- predict(model_both_2, testing_dataset)
# Computing model performance metrics
metrics_both_2 <- data.frame( R2 = R2(predictions_2, testing_dataset$SEED),
            RMSE = RMSE(predictions_2, testing_dataset$SEED),
            MAE = MAE(predictions_2, testing_dataset$SEED))

Model<- as.vector(c(
  'Empty',
  'Full',
  'Stepwise_both_1',
  'Stepwise_both_2')
)

`Mean_Average_Error`<- as.vector(c(
round(metrics_empty$MAE,digits=2),
round(metrics_full$MAE,digits=2),
round(metrics_both_2$MAE,digits=2),
round(metrics_both_1$MAE,digits=2))
)

metrics_all <- data.frame(Model, `Mean_Average_Error`)
metrics_table <- metrics_all %>%
  kbl(caption = "Mean Average Error by Model") %>%
  kable_classic(full_width = F, html_font = "Cambria")
metrics_table

#model_1: SEED ~ -4.33766090 + -0.82986380* WAB + 7.99275351* W_ratio + -0.07521933* G + -0.29202360* ADJOE + 14.16200872* BARTHAG + 0.33506762* ADJDE
#model_2: SEED~ -6.44869443+ -0.82091537* WAB + 8.24557772* W_ratio + 0.17145355* two_P_D + 15.13874804* BARTHAG + -0.30972253* ADJOE + 0.36305352* ADJDE + -0.07591390* ADJ_T + 0.03701627* FTRD + -0.13172055* EFG_D

```

I notice that the last model is the most successful at minimizing errors in the testing data. For this model, the mean average error (MAE) is the lowest (MAE=1.26) compared to other models. To further compare each model's performance of predicting the `SEED` on the training data, I draw *Predicted vs. Observed* plots as follows:

```{r, fig.align='center', fig.height=8, fig.width=8,echo=FALSE}
#将test model的数据点绘制在图上，观察是否拟合(可参考STOR320的final paper)
plot_both_1 <- ggplot(testing_dataset, aes(x=predict(model_both_1,newdata = testing_dataset), y= testing_dataset$SEED)) +
  geom_point() +
  geom_abline(intercept=0, slope=1,color="red") +
  labs(x='Predicted Values', y='Actual Values', title='Predicted vs. Actual Values (Both-direction Stepwise Selection Model 2)') +
  theme_bw()

plot_both_2 <- ggplot(testing_dataset, aes(x=predict(model_both_2,newdata = testing_dataset), y= testing_dataset$SEED)) +
  geom_point() +
  geom_abline(intercept=0, slope=1,color="red") +
  labs(x='Predicted Values', y='Actual Values', title='Predicted vs. Actual Values (Both-direction Stepwise Selection Model 1)') +
  theme_bw()

plot_full <- ggplot(testing_dataset, aes(x=predict(model_full,newdata = testing_dataset), y= testing_dataset$SEED)) +
  geom_point() +
  geom_abline(intercept=0, slope=1,color="red") +
  labs(x='Predicted Values', y='Actual Values', title='Predicted vs. Actual Values (Full Model)') +
  theme_bw()

plot_empty <- ggplot(testing_dataset, aes(x=predict(model_empty,newdata = testing_dataset), y= testing_dataset$SEED)) +
  geom_point() +
  geom_abline(intercept=0, slope=1,color="red") +
  labs(x='Predicted Values', y='Actual Values', title='Predicted vs. Actual Values (Empty Model)') +
  theme_bw()


grid.arrange(plot_both_1,plot_both_2,plot_full,plot_empty,ncol=2,nrow=2)
```

Although the first three plots look somewhat similar, we can roughly identify that the points in the first picture fit the red line (y=x) best, illustrating that the *Both-direction Stepwise Selection Model 2* (the last model mentioned above) performs the best at predicting the future `SEED` based on the existing data. And the model is -4.33766090 + -0.82986380* `WAB` + 7.99275351* `W_ratio` + -0.07521933* `G` + -0.29202360* `ADJOE` + 14.16200872* `BARTHAG` + 0.33506762* `ADJDE`.


# Conclusion and Discussion

```{r}


```





